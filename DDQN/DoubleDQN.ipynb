{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definitions of the hyper-parameters and configurations.\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Leaning rate\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# The total number of training episodes.\n",
    "TOTAL_EPISODE = 2000\n",
    "\n",
    "# (Linear) Decayed epsilon-greedy is used.\n",
    "START_EPSILON = 0.99\n",
    "END_EPSILON = 0.05\n",
    "EPSILON_DACAY_RATE = 0.02\n",
    "\n",
    "# Gamma value in RL theory\n",
    "GAMMA = 0.999\n",
    "\n",
    "# The size of the expirence pool.\n",
    "POOL_SIZE = 10000\n",
    "\n",
    "# Target net update interval episodes\n",
    "TARGET_UPDATE_INTERVAL = 2\n",
    "\n",
    "# Render or not\n",
    "DO_RENDERING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network structure. A fully-connected network should be enough.\n",
    "# Inputs are state (dimension is 4); outputs are Q values of the two actions in current state (dimension is 2).\n",
    "# NOTE: Large networks are very likely to degenerate, and perform much worse than the tiny networks do.\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.add_module('layer1', nn.Linear(4, 20))\n",
    "        self.add_module('layer2', nn.Linear(20, 20))\n",
    "        self.add_module('layer3', nn.Linear(20, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # NOTE: Cuz the network is tiny, in the case of using ReLU, alive nerons will be too few to make the network work.\n",
    "        x = F.leaky_relu_(self.layer1(x))\n",
    "        x = F.leaky_relu_(self.layer2(x))\n",
    "        return F.leaky_relu_(self.layer3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The expirence pool, which is vital for DQN.\n",
    "# The pool here is a ring buffer.\n",
    "class ExpPool(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pool = []\n",
    "        self.index = 0\n",
    "\n",
    "    def add(self, state, action, next_state, reward):\n",
    "        instance = Instance(state, action, next_state, reward)\n",
    "        if len(self.pool) < POOL_SIZE:\n",
    "            self.pool.append(instance)\n",
    "        else:\n",
    "            self.pool[self.index] = instance\n",
    "            \n",
    "        self.index = (self.index + 1) % POOL_SIZE\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.pool, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.pool)\n",
    "\n",
    "# One single instance in the pool.\n",
    "# To use TD, the following 4 things is required.\n",
    "class Instance(object):\n",
    "    \n",
    "    def __init__(self, state, action, next_state, reward):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the initializations and declarations before taining.\n",
    "\n",
    "# Enviroment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Device\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Double DQN can make the training process more stable.\n",
    "policy_net = Network().to(device)\n",
    "target_net = Network().to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Experience pool\n",
    "exp_pool = ExpPool()\n",
    "\n",
    "# Data used to draw plots\n",
    "episode_total_loss = 0\n",
    "avg_loss_array = []\n",
    "episode_length_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of functions\n",
    "\n",
    "\n",
    "# The policy of the agent\n",
    "def epsilon_greedy(state, current_episode):\n",
    "\n",
    "    # Linear-decayed epsilon\n",
    "    epsilon = max(START_EPSILON - current_episode * EPSILON_DACAY_RATE,\n",
    "                  END_EPSILON)\n",
    "\n",
    "    # Take the action whose Q value is larger, or randomly.\n",
    "    # Since the network are only used to interact with enviroment will not be updated, we use torch.no_grad() here.\n",
    "    if random.random() > epsilon:\n",
    "        with torch.no_grad():\n",
    "            result = policy_net(\n",
    "                torch.tensor(state, device=device, dtype=torch.float))\n",
    "            return torch.max(result, 0)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Update model for one single time.\n",
    "def update():\n",
    "\n",
    "    # Do the update only if enough samples in the pool\n",
    "    if exp_pool.size() < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    # Prepare the samples, convert them into tensors.\n",
    "    states, actions, rewards, next_states = [], [], [], []\n",
    "    final_states, final_actions, final_rewards = [], [], []\n",
    "    instances = exp_pool.sample(BATCH_SIZE)\n",
    "    for instance in instances:\n",
    "        if instance.next_state is None:\n",
    "            final_states.append(instance.state)\n",
    "            final_actions.append(instance.action)\n",
    "            final_rewards.append(instance.reward)\n",
    "        else:\n",
    "            states.append(instance.state)\n",
    "            actions.append(instance.action)\n",
    "            rewards.append(instance.reward)\n",
    "            next_states.append(instance.next_state)\n",
    "\n",
    "    state_batch = torch.tensor(states + final_states,\n",
    "                               device=device,\n",
    "                               dtype=torch.float)\n",
    "    action_batch = torch.tensor(actions + final_actions,\n",
    "                                device=device,\n",
    "                                dtype=torch.long).view(-1, 1)\n",
    "    reward_batch = torch.tensor(rewards + final_rewards,\n",
    "                                device=device,\n",
    "                                dtype=torch.float)\n",
    "    next_state_batch = torch.tensor(next_states,\n",
    "                                    device=device,\n",
    "                                    dtype=torch.float)\n",
    "\n",
    "    # Get the Q values of the state-action pairs in the samples. Use policy net here.\n",
    "    policy_result = policy_net(state_batch)\n",
    "    q_values = torch.gather(policy_result, dim=1,\n",
    "                            index=action_batch).view(1, -1)[0]\n",
    "\n",
    "    # Get the state values of the next states (i.e., the max Q value of the next state). Use target net here.\n",
    "    # NOTE: the final states are vital, whose state values MUST be 0.\n",
    "    # Since the policy net will not be updated using gradient, we use detach() to stop the backprop.\n",
    "    next_state_values = torch.cat(\n",
    "        (torch.max(target_net(next_state_batch),\n",
    "                   1)[0].detach(), torch.zeros(len(final_states))), 0)\n",
    "\n",
    "    # Get the target Q value (according to TD).\n",
    "    target_q_values = GAMMA * next_state_values + reward_batch\n",
    "\n",
    "    # Calculate the loss function. We can try different of regression loss functions.\n",
    "    loss = nn.MSELoss()(q_values, target_q_values)\n",
    "    global episode_total_loss\n",
    "    episode_total_loss += loss\n",
    "\n",
    "    # Update the model.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# Draw the analysis plots.\n",
    "def draw_plot():\n",
    "    plt.figure(figsize=(50,20))\n",
    "    plt.xticks(fontsize=25)\n",
    "    plt.yticks(fontsize=25)\n",
    "    plt.plot(avg_loss_array)\n",
    "    plt.ylabel('loss', fontsize=35)\n",
    "    plt.xlabel('episode', fontsize=35)\n",
    "    plt.draw()\n",
    "\n",
    "    plt.figure(figsize=(50,20))\n",
    "    plt.xticks(fontsize=25)\n",
    "    plt.yticks(fontsize=25)\n",
    "    plt.plot(episode_length_array)\n",
    "    plt.ylabel('length', fontsize=35)\n",
    "    plt.xlabel('episode', fontsize=35)\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training process.\n",
    "for episode in range(TOTAL_EPISODE):\n",
    "\n",
    "    # Get the init state\n",
    "    state = env.reset()\n",
    "\n",
    "    for step in itertools.count():\n",
    "\n",
    "        # Take an action.\n",
    "        action = epsilon_greedy(state, episode)\n",
    "        new_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        # Update the model.\n",
    "        update()\n",
    "\n",
    "        if DO_RENDERING:\n",
    "            env.render()\n",
    "\n",
    "        if done:\n",
    "            # Save the final state sample into pool.\n",
    "            exp_pool.add(state, action, None, 0)\n",
    "            \n",
    "            # Record data.\n",
    "            avg_loss_array.append(episode_total_loss / (step + 1))\n",
    "            episode_length_array.append(step + 1)\n",
    "            episode_total_loss = 0\n",
    "\n",
    "            # Update the target net.\n",
    "            if 0 == episode % TARGET_UPDATE_INTERVAL:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            # Save the sample into pool.\n",
    "            exp_pool.add(state, action, new_state, reward)\n",
    "            state = new_state\n",
    "\n",
    "# Draw the plots.\n",
    "draw_plot()\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}